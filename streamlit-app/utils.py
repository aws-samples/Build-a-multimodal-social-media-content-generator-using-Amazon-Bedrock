import re
import io
import base64
import json
import boto3 
from PIL import Image
from io import BytesIO
import logging
from sagemaker.s3 import S3Downloader as s3down
from constants import *
import multiprocessing


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

bedrock = boto3.client(service_name='bedrock-runtime')

# a function to check the uploaded image size, if the width is bigger than 1408, then resize it to 1408

def resize_webcam_image(webcam_img):
    if webcam_img.width > 1408:
        webcam_img = webcam_img.resize((1408, int(webcam_img.height * 1408 / webcam_img.width)))
    if webcam_img.width < 256:
        webcam_img = webcam_img.resize((256, int(webcam_img.height * 256 / webcam_img.width)))
    if webcam_img.height < 256:
        webcam_img = webcam_img.resize((int(webcam_img.width * 256 / webcam_img.height), 256))
    return webcam_img


def resize_image(upload_file):
    image_bytes = upload_file.getvalue()
    image = Image.open(BytesIO(image_bytes))
    if image.width > 1408:
        image = image.resize((1408, int(image.height * 1408 / image.width)))
    return image

def image_to_base64(image_path=None, upload_file=None):
    """
    Convert an image file to a base64 string.

    Args:
        image_path (str): The path to the image file.
        upload_file : The image file uploaded by user

    Returns:
        str: The base64 encoded string representation of the image.
    """
    if image_path:
        with open(image_path, "rb") as img_file:
            image_bytes = img_file.read()
            base64_str = base64.b64encode(image_bytes).decode("utf-8")
            
    if upload_file:
        with BytesIO() as byte_io:
            upload_file.save(byte_io, format="PNG")
            upload_file_bytes = byte_io.getvalue()
        base64_str = base64.b64encode(upload_file_bytes).decode('utf-8')
        
    return base64_str

def base64_to_image(base64_str):
    """
    Convert a base64 string to an image object.

    Args:
        base64_str (str): The base64 encoded string representing the image.

    Returns:
        Image: The PIL Image object.
    """
    image_bytes = base64.b64decode(base64_str)
    image = Image.open(BytesIO(image_bytes))
    return image


def get_titan_ai_request_body(outpaint_prompt, negative_prompt, mask_prompt, image_str=None):
    # see https://platform.stability.ai/docs/features/api-parameters
    import random

    seed = random.randint(0, 2147483647)
    body = {
        "taskType": "OUTPAINTING",
        "outPaintingParams": {
            "text": outpaint_prompt,
            "negativeText": negative_prompt,
            "image": image_str,
            "maskPrompt": mask_prompt,
            # "maskImage": image_to_base64(mask),
            "outPaintingMode": "PRECISE" # or DEFAULT
        },
        "imageGenerationConfig": {
            "numberOfImages": 1,
            "quality": "premium",
            "cfgScale": 8,
            "seed": seed,
        }
    }
    return json.dumps(body)



def generate_image(model_id, body):
    """
    Generate an image using SDXL 1.0 or Titan on demand.
    Args:
    model_id (str): The model ID to use.
    body (str) : The request body to use.
    Returns:
    image_bytes (bytes): The image generated by the model.
    """
    logger.info("Generating image with model %s", model_id)
    
    accept = "application/json"
    content_type = "application/json"
    
    response = bedrock.invoke_model(
        body=body, modelId=model_id, accept=accept, contentType=content_type
    )
    response_body = json.loads(response.get("body").read())

    return response_body


def post_process_answer(response:str)->str:
    """
    Extracts the answer from the given response string.

    Args:
        response (str): The response string containing the answer.

    Returns:
        str: The extracted answer.
    """
    try:
        answer = re.findall(r'<answer>(.*?)</answer>', response, re.DOTALL)
        return answer[0]
    except:
        print("ERROR Unable to parse answer")
        # raise
        return ""

def generate_vision_answer(bedrock:boto3.client, messages:list, model_id:str, claude_config:dict,system_prompt:str):
    """
    Generates a vision answer using the specified model and configuration.
    
    Parameters:
    - bedrock_rt (boto3.client): The Bedrock runtime client.
    - messages (list): A list of messages.
    - model_id (str): The ID of the model to use.
    - claude_config (dict): The configuration for Claude.
    - system_prompt (str): The system prompt.
    
    Returns:
    - str: The formatted response.
    """
    
    body={'messages': [messages],**claude_config, "system": system_prompt}
    bedrock = boto3.client(service_name='bedrock-runtime')
    
    response = bedrock.invoke_model(modelId=model_id, body=json.dumps(body))   
    response = json.loads(response['body'].read().decode('utf-8'))
    print("Claude vision answer OK")
    formated_response= post_process_answer(response['content'][0]['text'])
    
    return formated_response


def get_titan_multimodal_embedding(
    image_bytes:str=None,  # maximum 2048 x 2048 pixels
    description:str=None, # English only and max input tokens 128
    dimension:int=1024,   # 1,024 (default), 384, 256
    model_id:str='amazon.titan-embed-image-v1'
):
    payload_body = {}
    embedding_config = {
        "embeddingConfig": { 
             "outputEmbeddingLength": dimension
         }
    }
    # You can specify either text or image or both
    if image_bytes:
        base64_image = base64.b64encode(image_bytes).decode('utf-8')
        payload_body["inputImage"] = base64_image

    if description:
        payload_body["inputText"] = description

    assert payload_body, "please provide either an image and/or a text description"
    # print("\n".join(payload_body.keys()))

    response = bedrock.invoke_model(
        body=json.dumps({**payload_body, **embedding_config}), 
        modelId=model_id,
        accept="application/json", 
        contentType="application/json"
    )

    return json.loads(response.get("body").read())


def get_image(file_name, dataset):

    item_idx = dataset.query(f"file_name == '{file_name}'").index[0]
    img_loc =  dataset.iloc[item_idx].local_path

    img = Image.open(img_loc)
    return img

def get_text(file_name, dataset):

    item_idx = dataset.query(f"file_name == '{file_name}'").index[0]
    text =  dataset.iloc[item_idx].text

    return text



def find_similar_items(image_bytes: str, query_prompt:str, k: int, num_results: int, index_name: str, dataset, open_search_client  ) -> []:
    """
    Main semantic search capability using knn on input image prompt.
    Args:
        k: number of top-k similar vectors to retrieve from OpenSearch index
        num_results: number of the top-k similar vectors to retrieve
        index_name: index name in OpenSearch
    """
    query_emb = get_titan_multimodal_embedding(image_bytes=image_bytes, description = query_prompt, dimension=1024)["embedding"]

    body = {
        "size": num_results,
        "_source": {
            "exclude": ["image_vector"],
        },
        "query": {
            "knn": {
                "image_vector": {
                    "vector": query_emb,
                    "k": k,
                }
            }
        },
    }     
        
    res = open_search_client.search(index=index_name, body=body)
    images = []
    texts = []
    

    for hit in res["hits"]["hits"]:
        id_ = hit["_id"]
        file_name = hit["_source"]["file_name"]
        post_text = hit["_source"]["post_text"]
        image = get_image(file_name = file_name, dataset = dataset)

        image.name_and_score = f'{hit["_score"]}:{hit["_source"]["file_name"]}'
        images.append(image)

        texts.append(f"Post Text: {post_text}")
            
            
    return images, texts

# def analyse_images_with_claude(images, prompt):
    
#     base_64_imgs = []
#     for img in images:
#         with BytesIO() as byte_io:
#             img.save(byte_io, format="PNG")
#             image_bytes = byte_io.getvalue()
#             img_b64 = base64.b64encode(image_bytes).decode()
#             base_64_imgs.append(img_b64)
        
#     messages=[
#         {
#             "role": "user",
#             "content": [
#                 {
#                     "type": "text",
#                     "text": "Image 1:"
#                 },
#                 {
#                     "type": "image",
#                     "source": {
#                         "type": "base64",
#                         "media_type": "image/jpeg",
#                         "data": base_64_imgs[0],
#                     },
#                 },
#                 {
#                     "type": "text",
#                     "text": "Image 2:"
#                 },
#                 {
#                     "type": "image",
#                     "source": {
#                         "type": "base64",
#                         "media_type": "image/jpeg",
#                         "data": base_64_imgs[1],
#                     },
#                 },
#                 {
#                     "type": "text",
#                     "text": "Image 3:"
#                 },
#                 {
#                     "type": "image",
#                     "source": {
#                         "type": "base64",
#                         "media_type": "image/jpeg",
#                         "data": base_64_imgs[2],
#                     },
#                 },
#                 {
#                     "type": "text",
#                     "text": prompt
#                 }
#             ],
#         }
#     ]
    
#     claude_config = {
#         'max_tokens': 1500, 
#         'temperature': 0, 
#         'anthropic_version': '',  
#         'top_p': 1, 
#         'stop_sequences': ['Human:']
#     }

#     body={'messages': messages, **claude_config, "system": SYSTEM_PROMPT}
#     bedrock = boto3.client(service_name='bedrock-runtime')
    
#     response = bedrock.invoke_model(modelId=MODEL_TEXT, body=json.dumps(body))   
#     response = json.loads(response['body'].read().decode('utf-8'))
#     print("Claude vision multi image answer OK")
#     formated_response = post_process_answer(response['content'][0]['text'])
    
#     return formated_response
    
def generate_text_with_claude(image, prompt):
    '''
    Generate text with Claude for post generation and historical posts analysis
    '''
    with BytesIO() as byte_io:
        image.save(byte_io, format="PNG")
        image_bytes = byte_io.getvalue()

    messages={"role": "user", "content": [
    {
            "type": "image",
            "source": {
            "type": "base64",
            "media_type": "image/jpeg",
            "data": base64.b64encode(image_bytes).decode(),
            }
    },
    {"type": "text", 
        "text": prompt}
    ]}
    

    claude_text = generate_vision_answer(bedrock, messages, MODEL_TEXT, CLAUDE_CONFIG, SYSTEM_PROMPT)   
    return claude_text


def parse_task_response(response:str)->str:
    """
    Extracts the task2_response from the given response string.

    Args:
        response (str): The response string containing the task2_response.

    Returns:
        str: The extracted task2_response.
    """
    try:
        task1_response = re.findall(r'<task1_response>(.*?)</task1_response>', response, re.DOTALL)
        task2_response = re.findall(r'<task2_response>(.*?)</task2_response>', response, re.DOTALL)
        
        return f'''**Scene Description**:  
                    {task1_response[0]}   

**Recommendations**:
{task2_response[0]}'''
    except:
        print("ERROR Unable to parse response from llm")
        return ""
    

def get_task2_response(response:str)->str:
    """
    Extracts the task2_response from the given response string.

    Args:
        response (str): The response string containing the task2_response.

    Returns:
        str: The extracted task2_response.
    """
    try:
        task2_response = re.findall(r'<task2_response>(.*?)</task2_response>', response, re.DOTALL)
        return task2_response[0]
    except:
        print("ERROR Unable to parse task2_response")
        return ""
    
def parse_response_multi_post_analysis(response, post_ids):
    res = []
    for post_id in post_ids:
        re_post = r'<{}>(.*?)</{}>'.format(post_id, post_id)
        try:
            post_response = re.findall(re_post, response, re.DOTALL)
            res.append(post_response[0])
        except:
            print(f"ERROR Unable to parse response for {post_id}")
            res.append("")
    return res
    
def process_images(_similar_items, PROMPT_ANALYSIS):
    pool = multiprocessing.Pool(processes=3)  # Create a pool of 3 worker processes
    args = [(image, PROMPT_ANALYSIS) for image in _similar_items[:3]]
    results = pool.starmap(generate_text_with_claude, args)  # Execute the function calls in parallel
    # Unpack the results
    analysis_text_0, analysis_text_1, analysis_text_2 = results
    # Close the pool and wait for the tasks to finish
    pool.close()
    pool.join()
    return analysis_text_0, analysis_text_1, analysis_text_2



def create_positioned_image(input_image, position):
    # Check if the input is a file path or image data
    if isinstance(input_image, str):
        # Open the input image from the file path
        input_image = Image.open(input_image)
    else:
         with io.BytesIO() as byte_arr:
            input_image.save(byte_arr, format='PNG')  # Specify the format if needed
            image_bytes = byte_arr.getvalue()
            image = Image.open(BytesIO(image_bytes))
 

    # Get the width and height of the input image
    input_width, input_height = input_image.size

    # Define the positions and their corresponding coordinates
    positions = {
        "top-left": (0, 0),
        "top-middle": (input_width, 0),
        "top-right": (input_width * 2, 0),
        "middle-left": (0, input_height),
        "middle-middle": (input_width, input_height),
        "middle-right": (input_width * 2, input_height),
        "bottom-left": (0, input_height * 2),
        "bottom-middle": (input_width, input_height * 2),
        "bottom-right": (input_width * 2, input_height * 2)
    }

    # Check if the provided position is valid
    if position not in positions:
        raise ValueError(f"Invalid position: {position}")

    # Get the position coordinates
    position_coords = positions[position]

    # Create a new blank image with triple the size of the input image
    output_width = input_width * 3
    output_height = input_height * 3
    output_image = Image.new("RGB", (output_width, output_height), color="black")

    # Paste the input image at the specified position
    output_image.paste(input_image, position_coords)
    
    resized_output_image = output_image.resize((input_width, input_height))

    return resized_output_image